from aws_cdk import (
    Stack,
    aws_ec2 as ec2,
    aws_ecs as ecs,
    aws_ecr as ecr,
    aws_iam as iam,
    aws_s3 as s3,
    aws_logs as logs,
    RemovalPolicy,
    Duration
)
from constructs import Construct

class EcsJavaProcessorStack(Stack):
    """
    CDK Stack for deploying a Synthea Java processor on ECS Fargate.
    
    This stack creates:
    - S3 bucket for storing output files
    - ECS Fargate cluster and task definition
    - ECR repository for Docker images
    - IAM roles with appropriate permissions
    - CloudWatch log group for task logs
    - Security group for network access
    
    Parameters:
    - vpc_id: (optional) Existing VPC ID to use. If not provided, uses default VPC.
    - s3_bucket_name: (optional) Existing S3 bucket name to use. If not provided, creates a new bucket.
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        vpc_id: str = None,
        s3_bucket_name: str = None,
        **kwargs
    ) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # ============================================================
        # S3 BUCKET - Stores output files generated by Synthea
        # ============================================================
        # Use provided S3 bucket or create a new one
        # If using existing bucket, ensure it exists and has proper permissions
        if s3_bucket_name:
            # Use existing S3 bucket
            output_bucket = s3.Bucket.from_bucket_name(
                self, "OutputBucket",
                bucket_name=s3_bucket_name
            )
        else:
            # Create new S3 bucket
            # Bucket name includes account ID to ensure global uniqueness
            # RETAIN policy prevents accidental deletion of data
            output_bucket = s3.Bucket(
                self, "OutputBucket",
                bucket_name=f"synthea-output-{self.account}",
                removal_policy=RemovalPolicy.RETAIN,  # Keep bucket when stack is deleted
                auto_delete_objects=False  # Don't delete objects on stack deletion
            )

        # ============================================================
        # VPC - Network configuration for ECS tasks
        # ============================================================
        # Use provided VPC ID or fall back to default VPC
        # VPC can be specified via context variable or parameter
        if vpc_id:
            # Use the specified VPC ID
            vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_id=vpc_id)
        else:
            # Fall back to default VPC
            vpc = ec2.Vpc.from_lookup(self, "VPC", is_default=True)
        
        # ============================================================
        # ECS CLUSTER - Logical grouping of ECS tasks
        # ============================================================
        cluster = ecs.Cluster(
            self, "JavaProcessorCluster",
            cluster_name="java-processor-cluster",
            vpc=vpc
        )

        # ============================================================
        # ECR REPOSITORY - Stores Docker images
        # ============================================================
        # RETAIN policy keeps images even if stack is deleted
        # Push your Docker image here after deployment
        repository = ecr.Repository(
            self, "JavaProcessorRepo",
            repository_name="java-processor",
            removal_policy=RemovalPolicy.RETAIN  # Keep images when stack is deleted
        )

        # ============================================================
        # IAM TASK ROLE - Permissions for the running container
        # ============================================================
        # This role is used by the container to access AWS services (S3)
        # The container assumes this role at runtime
        task_role = iam.Role(
            self, "TaskRole",
            assumed_by=iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
            description="Role for ECS task to access S3"
        )
        
        # Grant read/write permissions to the output bucket
        output_bucket.grant_read_write(task_role)

        # ============================================================
        # IAM EXECUTION ROLE - Permissions for ECS to manage the task
        # ============================================================
        # This role is used by ECS to pull images from ECR and write logs
        # Different from task_role which is used by the container itself
        execution_role = iam.Role(
            self, "ExecutionRole",
            assumed_by=iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AmazonECSTaskExecutionRolePolicy"
                )
            ]
        )

        # ============================================================
        # TASK DEFINITION - Blueprint for running the container
        # ============================================================
        # Defines CPU, memory, and roles for Fargate tasks
        # Adjust cpu and memory_limit_mib based on Synthea workload requirements
        task_definition = ecs.FargateTaskDefinition(
            self, "TaskDefinition",
            family="java-processor-task",
            cpu=1024,  # 1 vCPU (256, 512, 1024, 2048, 4096)
            memory_limit_mib=2048,  # 2 GB RAM (512 to 30720 in increments)
            task_role=task_role,  # Role for container to access AWS services
            execution_role=execution_role  # Role for ECS to manage the task
        )

        # ============================================================
        # CLOUDWATCH LOG GROUP - Stores container logs
        # ============================================================
        # Logs are retained for 1 week by default
        # Change retention period as needed for compliance/debugging
        log_group = logs.LogGroup(
            self, "LogGroup",
            log_group_name="/ecs/java-processor",
            removal_policy=RemovalPolicy.RETAIN,  # Keep logs when stack is deleted
            retention=logs.RetentionDays.ONE_WEEK  # Adjust retention as needed
        )

        # ============================================================
        # CONTAINER DEFINITION - Specifies the Docker container
        # ============================================================
        # This is where the Synthea JAR runs
        # Environment variables are passed to the container at runtime
        container = task_definition.add_container(
            "JavaProcessorContainer",
            container_name="java-processor",
            # Image must be pushed to ECR before running tasks
            image=ecs.ContainerImage.from_ecr_repository(repository, "latest"),
            # Send container stdout/stderr to CloudWatch
            logging=ecs.LogDrivers.aws_logs(
                stream_prefix="ecs",
                log_group=log_group
            ),
            # Environment variables available to the container
            # These are read by entrypoint.sh to configure S3 upload
            environment={
                "S3_BUCKET": output_bucket.bucket_name,  # Where to upload output
                "S3_PREFIX": "output"  # S3 key prefix for organization
            }
        )

        # ============================================================
        # SECURITY GROUP - Network firewall rules
        # ============================================================
        # Allows all outbound traffic (needed for S3 and ECR access)
        # No inbound rules needed since this is a batch processing task
        security_group = ec2.SecurityGroup(
            self, "TaskSecurityGroup",
            vpc=vpc,
            description="Security group for Java processor ECS tasks",
            allow_all_outbound=True  # Required for S3, ECR, and CloudWatch access
        )

        # ============================================================
        # STACK OUTPUTS - Make resources accessible to other code
        # ============================================================
        # These can be referenced by run-task.sh and trigger-via-api.py
        self.output_bucket = output_bucket
        self.cluster = cluster
        self.task_definition = task_definition
        self.repository = repository
        self.security_group = security_group

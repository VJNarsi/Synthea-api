from aws_cdk import (
    Stack,
    aws_ec2 as ec2,
    aws_batch as batch,
    aws_ecs as ecs,
    aws_ecr as ecr,
    aws_iam as iam,
    aws_s3 as s3,
    aws_logs as logs,
    RemovalPolicy,
    CfnOutput
)
from constructs import Construct

class BatchJavaProcessorStack(Stack):
    """
    CDK Stack for deploying a Synthea Java processor on AWS Batch.
    
    This stack creates:
    - S3 bucket for storing output files
    - AWS Batch compute environment (Fargate)
    - AWS Batch job queue and job definition
    - ECR repository for Docker images
    - IAM roles with appropriate permissions
    - CloudWatch log group for job logs
    - Security group for network access
    
    Parameters:
    - vpc_id: (optional) Existing VPC ID to use. If not provided, uses default VPC.
    - s3_bucket_name: (optional) Existing S3 bucket name to use. If not provided, creates a new bucket.
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        vpc_id: str = None,
        s3_bucket_name: str = None,
        **kwargs
    ) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # ============================================================
        # S3 BUCKET - Stores output files generated by Synthea
        # ============================================================
        # Use provided S3 bucket or create a new one
        # If using existing bucket, ensure it exists and has proper permissions
        if s3_bucket_name:
            # Use existing S3 bucket
            output_bucket = s3.Bucket.from_bucket_name(
                self, "OutputBucket",
                bucket_name=s3_bucket_name
            )
        else:
            # Create new S3 bucket
            # Bucket name includes account ID to ensure global uniqueness
            # RETAIN policy prevents accidental deletion of data
            output_bucket = s3.Bucket(
                self, "OutputBucket",
                bucket_name=f"synthea-output-{self.account}",
                removal_policy=RemovalPolicy.RETAIN,  # Keep bucket when stack is deleted
                auto_delete_objects=False  # Don't delete objects on stack deletion
            )

        # ============================================================
        # VPC - Network configuration for Batch jobs
        # ============================================================
        # Use provided VPC ID or fall back to default VPC
        # VPC can be specified via context variable or parameter
        if vpc_id:
            # Use the specified VPC ID
            vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_id=vpc_id)
        else:
            # Fall back to default VPC
            vpc = ec2.Vpc.from_lookup(self, "VPC", is_default=True)

        # ============================================================
        # ECR REPOSITORY - Stores Docker images
        # ============================================================
        # RETAIN policy keeps images even if stack is deleted
        # Push your Docker image here after deployment
        repository = ecr.Repository(
            self, "JavaProcessorRepo",
            repository_name="java-processor",
            removal_policy=RemovalPolicy.RETAIN  # Keep images when stack is deleted
        )

        # ============================================================
        # SECURITY GROUP - Network firewall rules
        # ============================================================
        # Allows all outbound traffic (needed for S3 and ECR access)
        # No inbound rules needed since this is a batch processing job
        security_group = ec2.SecurityGroup(
            self, "BatchSecurityGroup",
            vpc=vpc,
            description="Security group for Java processor Batch jobs",
            allow_all_outbound=True  # Required for S3, ECR, and CloudWatch access
        )

        # ============================================================
        # IAM JOB ROLE - Permissions for the running container
        # ============================================================
        # This role is used by the container to access AWS services (S3)
        # The container assumes this role at runtime
        job_role = iam.Role(
            self, "JobRole",
            assumed_by=iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
            description="Role for Batch job to access S3"
        )
        
        # Grant read/write permissions to the output bucket
        output_bucket.grant_read_write(job_role)

        # ============================================================
        # IAM EXECUTION ROLE - Permissions for Batch to manage the job
        # ============================================================
        # This role is used by Batch to pull images from ECR and write logs
        # Different from job_role which is used by the container itself
        execution_role = iam.Role(
            self, "ExecutionRole",
            assumed_by=iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AmazonECSTaskExecutionRolePolicy"
                )
            ]
        )

        # ============================================================
        # CLOUDWATCH LOG GROUP - Stores container logs
        # ============================================================
        # Logs are retained for 1 week by default
        # Change retention period as needed for compliance/debugging
        log_group = logs.LogGroup(
            self, "LogGroup",
            log_group_name="/aws/batch/java-processor",
            removal_policy=RemovalPolicy.RETAIN,  # Keep logs when stack is deleted
            retention=logs.RetentionDays.ONE_WEEK  # Adjust retention as needed
        )

        # ============================================================
        # BATCH COMPUTE ENVIRONMENT - Defines compute resources
        # ============================================================
        # Uses Fargate for serverless compute (no EC2 instances to manage)
        # Batch automatically scales based on job queue demand
        compute_environment = batch.FargateComputeEnvironment(
            self, "ComputeEnvironment",
            compute_environment_name="java-processor-compute-env",
            vpc=vpc,
            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC),
            security_groups=[security_group]
        )

        # ============================================================
        # BATCH JOB QUEUE - Queue for submitting jobs
        # ============================================================
        # Jobs submitted to this queue will run on the compute environment
        # Priority determines which queue gets resources first (higher = first)
        job_queue = batch.JobQueue(
            self, "JobQueue",
            job_queue_name="java-processor-queue",
            priority=1,
            compute_environments=[
                batch.JobQueueComputeEnvironment(
                    compute_environment=compute_environment,
                    order=1
                )
            ]
        )

        # ============================================================
        # BATCH JOB DEFINITION - Blueprint for running the container
        # ============================================================
        # Defines the Docker image, CPU, memory, and environment variables
        # Adjust cpu and memory based on Synthea workload requirements
        job_definition = batch.EcsJobDefinition(
            self, "JobDefinition",
            job_definition_name="java-processor-job",
            container=batch.EcsFargateContainerDefinition(
                self, "Container",
                image=ecs.ContainerImage.from_ecr_repository(repository, "latest"),
                cpu=1024,  # 1 vCPU (0.25, 0.5, 1, 2, 4, 8, 16)
                memory=2048,  # 2 GB RAM (512 to 30720 in increments)
                job_role=job_role,  # Role for container to access AWS services
                execution_role=execution_role,  # Role for Batch to manage the job
                log_configuration=ecs.LogDriver.aws_logs(
                    stream_prefix="batch",
                    log_group=log_group
                ),
                environment={
                    "S3_BUCKET": output_bucket.bucket_name,  # Where to upload output
                    "S3_PREFIX": "output"  # S3 key prefix for organization
                }
            )
        )

        # ============================================================
        # STACK OUTPUTS - Make resources accessible to other code
        # ============================================================
        # These can be referenced by run-job.sh and trigger-via-api.py
        self.output_bucket = output_bucket
        self.job_queue = job_queue
        self.job_definition = job_definition
        self.repository = repository
        self.security_group = security_group
        
        # Output key resource names for easy reference
        CfnOutput(self, "JobQueueName", value=job_queue.job_queue_name)
        CfnOutput(self, "JobDefinitionName", value=job_definition.job_definition_name)
        CfnOutput(self, "RepositoryUri", value=repository.repository_uri)
        CfnOutput(self, "BucketName", value=output_bucket.bucket_name)

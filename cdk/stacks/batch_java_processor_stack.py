from aws_cdk import (
    Stack,
    Size,
    aws_ec2 as ec2,
    aws_batch as batch,
    aws_ecs as ecs,
    aws_ecr as ecr,
    aws_iam as iam,
    aws_s3 as s3,
    aws_codebuild as codebuild,
    RemovalPolicy,
    CfnOutput
)
from constructs import Construct

class BatchJavaProcessorStack(Stack):
    """
    CDK Stack for deploying a Synthea Java processor on AWS Batch.
    
    This stack creates:
    - S3 bucket for storing output files
    - AWS Batch compute environment (Fargate)
    - AWS Batch job queue and job definition
    - ECR repository for Docker images
    - IAM roles with appropriate permissions
    - CloudWatch log group for job logs
    - Security group for network access
    
    Parameters:
    - vpc_id: (optional) Existing VPC ID to use. If not provided, uses default VPC.
    - s3_bucket_name: (optional) Existing S3 bucket name to use. If not provided, creates a new bucket.
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        vpc_id: str = None,
        s3_bucket_name: str = None,
        **kwargs
    ) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # ============================================================
        # S3 BUCKET - Stores output files generated by Synthea
        # ============================================================
        # Use provided S3 bucket or create a new one
        # If using existing bucket, ensure it exists and has proper permissions
        if s3_bucket_name:
            # Use existing S3 bucket
            output_bucket = s3.Bucket.from_bucket_name(
                self, "OutputBucket",
                bucket_name=s3_bucket_name
            )
        else:
            # Create new S3 bucket
            # Bucket name includes account ID to ensure global uniqueness
            # RETAIN policy prevents accidental deletion of data
            output_bucket = s3.Bucket(
                self, "OutputBucket",
                bucket_name=f"synthea-output-{self.account}",
                removal_policy=RemovalPolicy.RETAIN,  # Keep bucket when stack is deleted
                auto_delete_objects=False  # Don't delete objects on stack deletion
            )

        # ============================================================
        # VPC - Network configuration for Batch jobs
        # ============================================================
        # Use provided VPC ID or fall back to default VPC
        # VPC can be specified via context variable or parameter
        if vpc_id:
            # Use the specified VPC ID
            vpc = ec2.Vpc.from_lookup(self, "VPC", vpc_id=vpc_id)
        else:
            # Fall back to default VPC
            vpc = ec2.Vpc.from_lookup(self, "VPC", is_default=True)

        # ============================================================
        # ECR REPOSITORY - Stores Docker images
        # ============================================================
        repository = ecr.Repository(
            self, "JavaProcessorRepo",
            repository_name="java-processor",
            removal_policy=RemovalPolicy.RETAIN
        )

        # ============================================================
        # S3 BUCKET FOR BUILD ARTIFACTS
        # ============================================================
        build_bucket = s3.Bucket(
            self, "BuildBucket",
            bucket_name=f"synthea-build-{self.account}",
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True
        )

        # ============================================================
        # CODEBUILD PROJECT - Builds and pushes Docker image
        # ============================================================
        codebuild_project = codebuild.Project(
            self, "DockerBuildProject",
            project_name="synthea-docker-build",
            environment=codebuild.BuildEnvironment(
                build_image=codebuild.LinuxBuildImage.STANDARD_7_0,
                privileged=True
            ),
            environment_variables={
                "ECR_REPO_URI": codebuild.BuildEnvironmentVariable(value=repository.repository_uri),
                "AWS_DEFAULT_REGION": codebuild.BuildEnvironmentVariable(value=self.region),
                "AWS_ACCOUNT_ID": codebuild.BuildEnvironmentVariable(value=self.account),
                "BUILD_BUCKET": codebuild.BuildEnvironmentVariable(value=build_bucket.bucket_name)
            },
            build_spec=codebuild.BuildSpec.from_object({
                "version": "0.2",
                "phases": {
                    "pre_build": {
                        "commands": [
                            "echo Downloading source from S3...",
                            "aws s3 cp s3://$BUILD_BUCKET/source.zip source.zip",
                            "unzip source.zip",
                            "echo Logging in to Amazon ECR...",
                            "aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
                        ]
                    },
                    "build": {
                        "commands": [
                            "echo Build started on `date`",
                            "echo Building the Docker image...",
                            "docker build -t $ECR_REPO_URI:latest ."
                        ]
                    },
                    "post_build": {
                        "commands": [
                            "echo Build completed on `date`",
                            "echo Pushing the Docker image...",
                            "docker push $ECR_REPO_URI:latest"
                        ]
                    }
                }
            })
        )
        
        repository.grant_pull_push(codebuild_project)
        build_bucket.grant_read(codebuild_project)

        # ============================================================
        # SECURITY GROUP - Network firewall rules
        # ============================================================
        # Allows all outbound traffic (needed for S3 and ECR access)
        # No inbound rules needed since this is a batch processing job
        security_group = ec2.SecurityGroup(
            self, "BatchSecurityGroup",
            vpc=vpc,
            description="Security group for Java processor Batch jobs",
            allow_all_outbound=True  # Required for S3, ECR, and CloudWatch access
        )

        # ============================================================
        # IAM JOB ROLE - Permissions for the running container
        # ============================================================
        # This role is used by the container to access AWS services (S3)
        # The container assumes this role at runtime
        job_role = iam.Role(
            self, "JobRole",
            assumed_by=iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
            description="Role for Batch job to access S3"
        )
        
        # Grant read/write permissions to the output bucket
        output_bucket.grant_read_write(job_role)

        # ============================================================
        # IAM EXECUTION ROLE - Permissions for Batch to manage the job
        # ============================================================
        # This role is used by Batch to pull images from ECR and write logs
        # Different from job_role which is used by the container itself
        execution_role = iam.Role(
            self, "ExecutionRole",
            assumed_by=iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AmazonECSTaskExecutionRolePolicy"
                )
            ]
        )



        # ============================================================
        # VPC ENDPOINTS - Allow private access to AWS services
        # ============================================================
        vpc.add_interface_endpoint(
            "EcrDockerEndpoint",
            service=ec2.InterfaceVpcEndpointAwsService.ECR_DOCKER,
            security_groups=[security_group]
        )
        vpc.add_interface_endpoint(
            "EcrApiEndpoint",
            service=ec2.InterfaceVpcEndpointAwsService.ECR,
            security_groups=[security_group]
        )
        vpc.add_interface_endpoint(
            "CloudWatchLogsEndpoint",
            service=ec2.InterfaceVpcEndpointAwsService.CLOUDWATCH_LOGS,
            security_groups=[security_group]
        )

        # ============================================================
        # BATCH COMPUTE ENVIRONMENT - Defines compute resources
        # ============================================================
        # Uses Fargate for serverless compute (no EC2 instances to manage)
        # Batch automatically scales based on job queue demand
        compute_environment = batch.FargateComputeEnvironment(
            self, "ComputeEnvironment",
            compute_environment_name="java-processor-compute-env",
            vpc=vpc,
            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC),
            security_groups=[security_group]
        )

        # ============================================================
        # BATCH JOB QUEUE - Queue for submitting jobs
        # ============================================================
        # Jobs submitted to this queue will run on the compute environment
        # Priority determines which queue gets resources first (higher = first)
        job_queue = batch.JobQueue(
            self, "JobQueue",
            job_queue_name="java-processor-queue",
            priority=1,
            compute_environments=[
                batch.OrderedComputeEnvironment(
                    compute_environment=compute_environment,
                    order=1
                )
            ]
        )

        # ============================================================
        # BATCH JOB DEFINITION - Blueprint for running the container
        # ============================================================
        # Defines the Docker image, CPU, memory, and environment variables
        # Adjust cpu and memory based on Synthea workload requirements
        job_definition = batch.EcsJobDefinition(
            self, "JobDefinition",
            job_definition_name="java-processor-job",
            container=batch.EcsFargateContainerDefinition(
                self, "Container",
                image=ecs.ContainerImage.from_ecr_repository(repository, "latest"),
                cpu=1,
                memory=Size.gibibytes(2),
                job_role=job_role,
                execution_role=execution_role,
                environment={
                    "S3_BUCKET": output_bucket.bucket_name,
                    "S3_PREFIX": "output"
                }
            )
        )

        # ============================================================
        # STACK OUTPUTS - Make resources accessible to other code
        # ============================================================
        # These can be referenced by run-job.sh and trigger-via-api.py
        self.output_bucket = output_bucket
        self.job_queue = job_queue
        self.job_definition = job_definition
        self.repository = repository
        self.security_group = security_group
        
        # Output key resource names for easy reference
        CfnOutput(self, "JobQueueName", value=job_queue.job_queue_name)
        CfnOutput(self, "JobDefinitionName", value=job_definition.job_definition_name)
        CfnOutput(self, "RepositoryUri", value=repository.repository_uri)
        CfnOutput(self, "BucketName", value=output_bucket.bucket_name)
        CfnOutput(self, "CodeBuildProjectName", value=codebuild_project.project_name)
        CfnOutput(self, "BuildBucketName", value=build_bucket.bucket_name)
